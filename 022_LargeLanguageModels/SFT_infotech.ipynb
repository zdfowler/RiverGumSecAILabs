{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f63434cd-dbe6-4964-a6c0-e0391c329a0d",
      "metadata": {
        "id": "f63434cd-dbe6-4964-a6c0-e0391c329a0d"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiverGumSecurity/AILabs/blob/main/022_LargeLanguageModels/SFT_infotech.ipynb\" target=\"_new\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb0ef625-6ea2-479c-bc50-008d69e5ed6c",
      "metadata": {
        "id": "bb0ef625-6ea2-479c-bc50-008d69e5ed6c"
      },
      "source": [
        "## Information Technology Dataset Supervised Fine Tuning of Llama3\n",
        "\n",
        "* Google colab does not have unsloth installed by default\n",
        "* Hugging Face API Key/Token will need to exist in keystore for colab or local disk otherwise\n",
        "* We set a max_seq_length parameter to set the token vector size.  Padding will occur if less than this size\n",
        "* When text streaming in Google Colab, wrapping does not work so we are forced to inject some CSS using an iPython callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "024e2f61-8970-47f4-a737-8c857c69c5ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "024e2f61-8970-47f4-a737-8c857c69c5ad",
        "outputId": "7c4fe6ea-92a5-4a8e-afa2-5c1377c9a27f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-29de1aff12a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install unsloth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Fix Xformers performance issues since 0.0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "try:\n",
        "    import unsloth\n",
        "except ImportError:\n",
        "    !pip install unsloth\n",
        "\n",
        "# suppresses some noisy warnings which are just annoying\n",
        "warnings.filterwarnings('ignore')\n",
        "max_seq_length = 4096\n",
        "\n",
        "# Setup Hugging Face Credentials.\n",
        "HF_APIKEY = ''\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import userdata\n",
        "    HF_APIKEY = userdata.get('HF_APIKEY')\n",
        "else:\n",
        "    with open(pathlib.Path.home() / '.hfkey') as hf:\n",
        "        HF_APIKEY = hf.read().strip()\n",
        "if not HF_APIKEY:\n",
        "    print('[-] ERROR: Cannot continue without Hugging Face API Key')\n",
        "    sys.exit(0)\n",
        "os.environ['HF_TOKEN'] = HF_APIKEY\n",
        "\n",
        "model, tokenizer = unsloth.FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, load_in_4bit = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEdcVkJq7hyw"
      },
      "id": "YEdcVkJq7hyw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDUGCVbY7iAr"
      },
      "id": "TDUGCVbY7iAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa00886a-fcda-43b2-a535-88a21a5d3e15",
      "metadata": {
        "id": "aa00886a-fcda-43b2-a535-88a21a5d3e15"
      },
      "outputs": [],
      "source": [
        "# This code fixes a word wrapping issue\n",
        "# in Google Co-Lab on Text Streamer output.\n",
        "from warnings import warn\n",
        "from IPython import get_ipython\n",
        "from IPython.display import HTML, display\n",
        "from transformers import TextStreamer\n",
        "\n",
        "def enable_word_wrap(*args, **kwargs):\n",
        "    display(HTML('''\n",
        "    <style>\n",
        "        pre {\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "    </style>\n",
        "    '''))\n",
        "\n",
        "def register_ipython_callback_once(event_name, cb):\n",
        "    ev = get_ipython().events\n",
        "    cb_unregs = [cb_old for cb_old in ev.callbacks[event_name] if cb_old.__name__ == cb.__name__]\n",
        "    if len(cb_unregs) == 1 and cb.__code__ == cb_unregs[0].__code__:\n",
        "        return\n",
        "    for cb_old in cb_unregs:\n",
        "        warn(f'Removing unexpected callback {cb_old}.')\n",
        "        ev.unregister(event_name, cb_old)\n",
        "    ev.register(event_name, cb)\n",
        "\n",
        "register_ipython_callback_once('pre_run_cell', enable_word_wrap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "438afad3-8bf6-4335-9072-1f86b0dddf57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "438afad3-8bf6-4335-9072-1f86b0dddf57",
        "outputId": "b10f0a8e-18ff-4889-fc40-0cb83d46671d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'unsloth' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-08dbfcca795a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PEFT MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = unsloth.FastLanguageModel.get_peft_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m## 8 or 16 is reasonable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlora_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m## 32 on a GPU with min 16GB Ram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'unsloth' is not defined"
          ]
        }
      ],
      "source": [
        "# PEFT MODEL\n",
        "model = unsloth.FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,               ## 8 or 16 is reasonable\n",
        "    lora_alpha = 32,      ## 32 on a GPU with min 16GB Ram\n",
        "    lora_dropout = 0.07,  ## between 0.05 and 1.0\n",
        "\n",
        "    ## best to leave below params alone\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d8c251-4593-4f78-a35b-2ee76535239f",
      "metadata": {
        "id": "23d8c251-4593-4f78-a35b-2ee76535239f"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def format_prompts(p):\n",
        "    # these are provided as lists\n",
        "    instructions = p['prompt']\n",
        "    outputs      = p['messages']\n",
        "    texts = []\n",
        "    for ins, outp in zip(instructions, outputs):\n",
        "        text = prompt.format(ins, '', outp[1]['content']) + tokenizer.eos_token\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"gussieIsASuccessfulWarlock/information_technology_instruct_mcq_2481\", split = \"train\")\n",
        "ds = ds.map(format_prompts, batched = True,)\n",
        "ds = ds.remove_columns([\"messages\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925a8e91-6e9f-47ac-a0ae-d8743f3dae44",
      "metadata": {
        "id": "925a8e91-6e9f-47ac-a0ae-d8743f3dae44"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "newmodel = model\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset = ds,\n",
        "    args = TrainingArguments(\n",
        "        ## hyper-params we usually can tune\n",
        "        learning_rate = 2e-4,             ## between 1e-4 and 5e-4\n",
        "        per_device_train_batch_size = 4,  ## 2, 4, or 8\n",
        "        gradient_accumulation_steps = 8,  ## 4 up to 8\n",
        "        num_train_epochs = 1,             ## max_steps overrides this\n",
        "        max_steps = 10,                   ## single Epoch / 50 steps or more\n",
        "        weight_decay = 0.01,              ## 0.01 is conservative and good for not overfitting\n",
        "\n",
        "        ## params below are typically not modified\n",
        "        warmup_steps = 5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d33002f-ef3e-482f-8c5b-7a2e684c1060",
      "metadata": {
        "id": "3d33002f-ef3e-482f-8c5b-7a2e684c1060"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de5f9fe-c0c6-4f03-89aa-64ca4a304516",
      "metadata": {
        "id": "8de5f9fe-c0c6-4f03-89aa-64ca4a304516"
      },
      "outputs": [],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory/max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11b104d-605f-4b7b-a32f-c56879e2264d",
      "metadata": {
        "id": "a11b104d-605f-4b7b-a32f-c56879e2264d"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub('l3-infotech')\n",
        "tokenizer.push_to_hub('l3-infotech')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590775dd-7161-405a-91b1-c297def53fb5",
      "metadata": {
        "id": "590775dd-7161-405a-91b1-c297def53fb5"
      },
      "outputs": [],
      "source": [
        "unsloth.FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        \"What is an IP address and subnet mask?\", # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4096)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e2e745-8e00-49bf-bdd3-57e5f1c7fa26",
      "metadata": {
        "id": "53e2e745-8e00-49bf-bdd3-57e5f1c7fa26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}