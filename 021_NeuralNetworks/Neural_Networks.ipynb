{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b028a26-f0e1-4c50-bad5-0544adf04233",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/RiverGumSecurity/AILabs/blob/main/notebooks/ModuleXX_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7318404-f202-4515-bc11-86022e59e744",
   "metadata": {},
   "source": [
    "# Supervised Learning and Neural Networks with Phishing Email Data\n",
    "\n",
    "Neural networks are a subset of machine learning algorithms inspired by the human brain's structure and function. They consist of layers of interconnected \"neurons\" that transform the input data through weighted connections to produce an output. Neural networks are particularly powerful for complex tasks like image recognition and natural language processing.\n",
    "\n",
    "Neural networks consist of an input layer, one or more hidden layers, and an output layer. These layers consist of \"neurons\" where each neuron processes input data by applying a weight and an activation function (like sigmoid or ReLU) to produce an output. The training (or learning) process adjusts the weights of the connections between neurons using algorithms like backpropagation and optimization techniques like gradient descent. Neural networks, particularly deep neural networks (deep learning), can model highly complex relationships between inputs and outputs by learning hierarchical feature representations in the hidden layers. Deep learning neural networks typically contain three or more hidden layers.\n",
    "\n",
    "<center><img src=\"https://images.edrawsoft.com/articles/neural-network-diagram/example1.png\" width=\"800\" height=\"600\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037f4ce-8b9a-444b-bdaa-0f36208eacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import os \n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,f1_score,classification_report,ConfusionMatrixDisplay,confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding,GRU,LSTM,Bidirectional,SimpleRNN\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6e503-b7a9-472a-b6f1-e00cd2284f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supress debug warnings in training output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppresses INFO and WARNING messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppresses INFO and WARNING messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982cbe9-eb9a-4f86-ae8b-ceccd31fedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed static value for consistent results\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e37ef-b7d4-42df-958a-440d1640f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect GPU device\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d6a82-d65f-4413-bff4-3f53aa366eba",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preparation\n",
    "\n",
    "The dataset that will be used to train and test the different models is from https://www.kaggle.com/datasets/subhajournal/phishingemails.  It is pre-labeled as \"Safe Email\" or \"Phishing Email\".  The data will be cleaned and prepared for training machine learning models with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc29abc-c9e0-4cbf-9c36-5d8e5506a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data into a Pandas dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/RiverGumSecurity/Datasets/refs/heads/main/Kaggle/Phishing_Email.csv.gz')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46408f-7be9-4b0e-9074-dc05def90709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information on the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cd602-2141-4cac-8de7-0ed41cde6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values in place, drop Unamed:0 column, drop duplicates\n",
    "df.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df.dropna(inplace=True,axis=0)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0e2d2-738b-4471-9514-f226de5d3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c8644-ab23-4e99-a6f6-17df7ddd2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cleaned dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b5b12-ab7a-4a85-ad4b-07b4b68af6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of Email types, a Safe Email or a Phishing Email\n",
    "df['Email Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bda6a-4af6-492a-93b8-aff225609d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot counts of Safe Email vs Phishing Email\n",
    "df['Email Type'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d52a3-db0e-4649-8700-e04fe6971217",
   "metadata": {},
   "source": [
    "Now we need to further process and clean the data. This is a binary classification problem, and we need to assign a label (a 1 or a 0) to the email categories. Then we will remove URLS and non word characters from the emails - we are interested in the similarities of the text itself.  Then we lowercase all of the characters, convert all multiple whitespace characters to single whitespace, and remove any trailing whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e191c8-4f2a-4878-a652-a0f0a82472d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Email Type\n",
    "lbl = LabelEncoder()\n",
    "df['Email Type'] = lbl.fit_transform(df['Email Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e153e-a279-4ee5-8e66-1153784a78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text.\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "df['Email Text']=df['Email Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5fb38-80a5-456b-8f4e-068e10c0bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc76d3-7e26-40ba-aa9a-715af4bf2788",
   "metadata": {},
   "source": [
    "\n",
    "Because our email messages are variable length, we need to fix the length of each vector we create. In deep learning, it’s often beneficial to have inputs of consistent shapes. Setting max_len ensures that all sequences are padded or truncated to a specific length, allowing you to batch-process sequences more efficiently. \n",
    "\n",
    "Next we define a simple neural network model using TensorFlow’s Keras Sequential API which is specifically designed for text data and allows you to stack layers in a linear sequence. Each added layer will be executed in the order it’s added, from the first layer to the last. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a669b-db0d-427a-aafb-782ee1a4e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set max_len\n",
    "max_len = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5beb8-cd63-446a-8ca1-ed2802361dae",
   "metadata": {},
   "source": [
    "We initalize a new [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class which is used for preprocessing text data by converting words or characters into numerical representations that a machine learning model can understand. In general, a token is a word or a character.  In our case, a token is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d6831-73dc-4062-a912-2d9d7a3878a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize new instance of Tokenizer class.\n",
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35190462-b1e6-4d0f-b5a2-2ce9c22d6133",
   "metadata": {},
   "source": [
    "The next step is to use `fit_on_texts` method in the 'Tokenizer' class to  processes each text entry in the `Email Text` column of the Pandas data frame to build a vocabulary, where each unique word (or token) is assigned a unique integer index based on its frequency across the entire dataset. It then creates a word index mapping each word (token) to a unique integer in each email. \n",
    "\n",
    "Then we use the `texts_to_sequences` method to create a list of integer sequence for each email from our word index mappings.  These sequence of integers are then converted to vectors of fixed length (256 from our max_len variable) and padded or truncated as necessary and stored in a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34b585-b929-4e2b-8b58-637d0f30dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "tk.fit_on_texts(df['Email Text'])\n",
    "sequences = tk.texts_to_sequences(df['Email Text'])\n",
    "vector = pad_sequences(sequences,padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e493b-0519-4a42-abff-f91f30093e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows in the vector numpy array, matches the number of emails in our dataset.\n",
    "vector.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2b0cd-64ca-457d-a108-d039a3c5176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of our word index\n",
    "len(tk.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95781310-2989-452f-9f9f-62cab6f11ac4",
   "metadata": {},
   "source": [
    "Now we set the `max_features` parameter to limit our overall vocabulary size to the most frequent words. There are two main reasons for this:\n",
    "\n",
    "- Reduces Memory Usage: In many NLP tasks, datasets contain thousands or millions of unique words. Limiting the vocabulary size to the most frequent words helps reduce memory and computational requirements, which is especially helpful when working with limited resources or large datasets.\n",
    "- Improves Model Generalization: High vocabulary sizes can lead to overfitting, as the model might memorize rare words that do not generalize well to new data. Setting max_features forces the model to focus on the most important and commonly occurring words, which generally improves performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b74f65-b35c-4ca9-90ef-5a3fdd801193",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c5a6e-3f69-4236-9e93-2516832b1fc0",
   "metadata": {},
   "source": [
    "Now we split our dataset into training and testing sets using the `train_test_split function` from scikit-learn.  We will train our neural network on 80 percent of the data, then evaluate the accuracy on the remaining 20 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2ee7a-bdfa-4a93-b6d3-98821a2f4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vector,df['Email Type'], test_size=0.2, random_state =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f624a6e-3341-4385-94af-a1935de22eca",
   "metadata": {},
   "source": [
    "## Training a Simple Recurrent Neural Network \n",
    "\n",
    "The [`SimpleRNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) layer in TensorFlow is a [recurrent neural network (RNN)]9https://en.wikipedia.org/wiki/Recurrent_neural_network) layer that processes sequential data, making it suitable for tasks like time series prediction or text generation. Each unit in SimpleRNN maintains a hidden state across time steps, allowing the model to \"remember\" previous inputs in the sequence. It supports configurations for returning only the final output or the entire sequence, and can also be stacked with other layers for deeper architectures. This layer is simpler and faster than more complex RNN types, like LSTM or GRU, but may struggle with longer sequences.\n",
    "\n",
    "The first line of code in the next block defines the Sequential model then we add the Embedding layer as the first layer of the model. The third layer is the RNN structure layer, then we add a dropout layer for regularization, and the last layer is a \"dense\" unit - this is where our binary classifcation probabilities will be output to.\n",
    "\n",
    "**Embedding Layer**\n",
    "\n",
    "The Embedding layer is used to convert input words (represented as integer indices) into dense vectors of fixed size. T\n",
    "\n",
    "`len(tk.word_index)+1` is the size of the vocabulary, where tk.word_index is the dictionary of words created by the Tokenizer. Adding 1 ensures that there’s an index for each word plus one additional index, often reserved for unknown or padding tokens.\n",
    "\n",
    "50 is the dimensionality of the word embeddings, meaning each word in the vocabulary will be represented by a 50-dimensional vector. This vector captures semantic relationships between words. The `units=100` parameter specifies the number of hidden units in the RNN, meaning the RNN will have 100 output features for each input sequence. Higher units can allow the RNN to capture more complex relationships in the data but can also increase computational requirements.\n",
    "\n",
    "**Simple RNN Layer**\n",
    "\n",
    "The Simple RNN layer to the modelprocesses the sequence of embeddings, maintaining a hidden state that captures dependencies between steps (words) in the sequence. \n",
    "\n",
    "**Dropout Layer** \n",
    "\n",
    "Dropout is a regularization technique that randomly sets 50% of the neurons in the layer to zero during each training step. This helps prevent overfitting by ensuring that the network doesn’t become too reliant on specific neurons and instead learns more generalized patterns.\n",
    "\n",
    "**Dense Layer**\n",
    "\n",
    "The final layer is a dense (fully connected) layer with a single unit.\n",
    "`1` specifies a single neuron because this model is for a binary classification task. The `activation='sigmoid'` seeting means the output will be a value between 0 and 1, representing a probability. In binary classification, this allows the network to output the probability of a sample belonging to a specific class (e.g., class 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca54a4d-2349-4ce0-ba6d-04c87231a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srnn = Sequential() # Sequential() API\n",
    "model_srnn.add(Embedding(len(tk.word_index)+1,50))\n",
    "model_srnn.add(SimpleRNN(units=100))\n",
    "model_srnn.add(Dropout(0.5))\n",
    "model_srnn .add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4acbf-3da3-44a9-9705-030f3b35d9de",
   "metadata": {},
   "source": [
    "Now we \"compile\" neural network model for training by specifying the loss function, optimizer, and evaluation metrics.\n",
    "\n",
    "The `binary crossentropy' loss function measures the difference between the predicted probability and the actual label (0 or 1). It is effective for binary classification tasks because it penalizes predictions that deviate from the true class probability, thus helping the model learn to distinguish between the two classes accurately.\n",
    "\n",
    "The `adam` (Adaptive Moment Estimation) optimizer is a widely used optimization algorithm that adjusts learning rates during training based on estimates of first and second moments of gradients. It combines the advantages of two other optimizers (AdaGrad and RMSProp).\n",
    "\n",
    "The `accuracy` metric provides an intuitive measure of model performance, especially in binary classification tasks, by indicating the percentage of correct predictions. This metric is tracked and displayed during training, helping to monitor the model’s performance over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21b47b-3223-43e4-bac2-0550a10d8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_srnn.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de46388-5693-4f5d-ba35-21fd15cbcadd",
   "metadata": {},
   "source": [
    "Next we train, or fit, the model to the data (x_train and y_train) for 20 epochs, with a batch size of 16 and evaluate the model on the validation set (x_test and y_test) after each epoch to monitor performance on unseen data.  \n",
    "\n",
    "Note that and epoch is one complete pass through the data set. The batch size is the number of samples processed before the model’s weights are updated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91d768-8d25-43d1-bd7a-1aad1fcdf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_srnn  = model_srnn.fit(x_train,y_train, epochs=20, batch_size=16, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e0c65-187b-424a-9067-72b1a64f6414",
   "metadata": {},
   "source": [
    "Now we plot the accuracy during training and the loss on the validation set of data. Essentially, we are visualizing how well a model is learning over time. so that we can gauge model performance and identify potential issues.\n",
    "\n",
    "<u>Training Loss Curve</u>\n",
    "\n",
    "**Expected Pattern:** The training loss should decrease over time, indicating that the model is improving its predictions on the training data.\n",
    "\n",
    "**Asymptotic Behavior:** As training progresses, the training loss typically decreases at a slower rate and eventually flattens out as the model approaches its best performance.\n",
    "\n",
    "**Signs of Overfitting:** If training loss continues to decrease while validation loss starts to increase or levels off, this suggests that the model is beginning to overfit to the training data.\n",
    "\n",
    "**Irregular Loss Patterns:** Sudden spikes or sharp oscillations might indicate issues like a high learning rate or instability, especially in models with non-convex loss landscapes like neural networks.\n",
    "\n",
    "<u>Validation Loss Curve</u>\n",
    "\n",
    "**Expected Pattern:**  Initially, validation loss should decrease along with training loss, showing that the model generalizes well to new data. After some point, however, it may begin to plateau or even increase, which is often a sign of overfitting.\n",
    "\n",
    "**Ideal Outcome:** A minimal gap between training and validation loss, with both curves reaching a plateau, generally indicates a well-fitting model.\n",
    "\n",
    "**Increasing Validation Loss:** If validation loss rises while training loss decreases, this suggests overfitting. Early stopping might be needed to halt training at the optimal point before overfitting worsens.\n",
    "\n",
    "<u>Training Accuracy Curve</u>\n",
    "\n",
    "**Expected Pattern:** Training accuracy should steadily increase as the model learns patterns in the training data. The curve typically starts lower and climbs until it approaches a high level, ideally plateauing near maximum accuracy for the dataset.\n",
    "\n",
    "**Unexpected Behavior:** If accuracy is not increasing, the model may be too simple (underfitting) or the learning rate may be set too low. On the other hand, if accuracy increases too quickly without validation accuracy following, it may indicate overfitting.\n",
    "\n",
    "<u>Validation Accuracy Curve</u>\n",
    "\n",
    "**Expected Pattern:** Like validation loss, validation accuracy should improve initially along with training accuracy. It should ideally track training accuracy closely, especially as training loss decreases.\n",
    "\n",
    "**Gap Between Training and Validation Accuracy:** A large gap where training accuracy is high but validation accuracy is low indicates overfitting. Efforts like regularization, data augmentation, or a more complex model might help in closing this gap.\n",
    "\n",
    "<u>Crossing or Diverging Curves</u>\n",
    "\n",
    "**Crossing Curves:** If validation loss and accuracy cross significantly (e.g., validation accuracy temporarily surpasses training accuracy), it may indicate randomness in validation performance due to small dataset size or other inconsistencies.\n",
    "\n",
    "**Diverging Curves:** If validation accuracy remains much lower than training accuracy, even as loss decreases, this often signals that the model is failing to generalize. Adding more data or using a simpler model might help.\n",
    "\n",
    "<u>Convergence Time</u>\n",
    "\n",
    "**Slow Convergence:** If the training loss and accuracy improve very slowly, consider adjusting hyperparameters like the learning rate or batch size.\n",
    "\n",
    "**Rapid Convergence:** If training loss decreases quickly but then stagnates, the model may have a high learning rate or too few epochs to fully learn the data patterns.\n",
    "\n",
    "<u>Regularization Effects</u>\n",
    "\n",
    "With Regularization (e.g., Dropout, L2): Training loss may show more fluctuation or be consistently higher compared to models without regularization. This is normal, as regularization prevents the model from memorizing training data too closely, helping validation loss stay low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0debcd-5aae-4804-a8f5-44ccbeb0e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(historical_srnn.history)\n",
    "\n",
    "pd.DataFrame(historical_srnn.history)[['accuracy', 'val_accuracy']].plot()\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "pd.DataFrame(historical_srnn.history)[['loss', 'val_loss']].plot()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af613c06-75d2-46c3-8aa5-a63e8fb5d3cb",
   "metadata": {},
   "source": [
    "We also create a confusion matrix like we did with the statistical learning models in the supervised learning lab notebook.  Recall that confusion matrix is a table that summarizes the performance of a classification model by showing the counts of correct and incorrect predictions for each class. It displays true positives, true negatives, false positives, and false negatives, helping to visualize errors and evaluate metrics like accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae132bf-45e3-4454-8cb6-c572297763ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_smp = model_srnn.predict(x_test)\n",
    "y_pred_smp = (y_pred_prob_smp > 0.5).astype(int)\n",
    "\n",
    "\n",
    "cnf_smp = confusion_matrix(y_test,y_pred_smp)\n",
    "ax_smp = ConfusionMatrixDisplay(confusion_matrix=cnf_smp,display_labels=['phishing','normal']).plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f7d61-ec2d-4da3-b3c8-78c328970e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "srnn_loss, srnn_accu = model_srnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "srnn_accu = (srnn_accu*100)\n",
    "\n",
    "# Display the accuracy value\n",
    "print(f'The accuracy of the SRNN model on the test data set was: {srnn_accu:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05ec1a-26d0-4ed2-b08a-b3485e7b0f84",
   "metadata": {},
   "source": [
    "## LTSM\n",
    "\n",
    "An LSTM (Long Short-Term Memory) neural network is a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data by using memory cells and gating mechanisms to retain or forget information over time. LSTMs are widely used for tasks involving time series data, natural language processing, and sequence prediction, such as forecasting stock prices, analyzing text sentiment, generating music, and translating languages, due to their ability to model and retain patterns over long sequences.\n",
    "\n",
    "We configure the layers of the model similar to the SRNN model - note the drastic improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61136d61-5da7-42a7-ae0a-3f2fd924b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential() # Sequential() API\n",
    "model_lstm.add(Embedding(len(tk.word_index)+1,50))\n",
    "model_lstm.add(LSTM(units=100))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc623ba-ed84-4f7e-9c4a-d0eec5e2f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca57d1a-2bc9-40f8-885f-3d274da3ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_lstm  = model_lstm.fit(x_train,y_train, epochs=20, batch_size=16, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c247a-aea8-4b6c-8922-08f1095311af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_results = model_lstm.evaluate(x_test, y_test)\n",
    "lstm_loss = lstm_results[0]  # Extract the loss from the results\n",
    "lstm_accu = (lstm_results[1]*100)  # Extract the accuracy from the results\n",
    "\n",
    "print(f\"Model Loss: {lstm_loss}\")\n",
    "print(f\"Model Accuracy: {lstm_accu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198118d1-a346-4099-9429-a1c1c31fbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model_lstm.predict(x_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc49587-9271-4168-aad0-c0d642073953",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(historical_lstm.history)\n",
    "\n",
    "pd.DataFrame(historical_lstm.history)[['accuracy', 'val_accuracy']].plot()\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "pd.DataFrame(historical_lstm.history)[['loss', 'val_loss']].plot()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fbd20-588f-4165-b84f-f2943bd751a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf = confusion_matrix(y_test,y_pred)\n",
    "ax = ConfusionMatrixDisplay(confusion_matrix=cnf,display_labels=['pishing','normal'])\n",
    "ax.plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeeef5d-62db-4b7f-8a92-9171dd0a0cb1",
   "metadata": {},
   "source": [
    "## Bidirectional\n",
    "\n",
    "A bidirectional neural network is a type of RNN that processes sequence data in both forward and backward directions, allowing it to capture context from both past and future states. This architecture is particularly useful in natural language processing tasks like speech recognition and text translation, where understanding the full context of a word or phrase requires information from both previous and subsequent elements in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659e4e9-c477-4e90-90a4-a82ff56b6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi = Sequential() # Sequential() API\n",
    "model_bi.add(Embedding(len(tk.word_index)+1,50))\n",
    "model_bi.add(Bidirectional(LSTM(units=100)))\n",
    "model_bi.add(Dropout(0.5))\n",
    "model_bi.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d79d71-a3c3-453a-996a-fa72376167cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bd43c-999d-4c95-9242-3cf1b569652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_bi  = model_bi.fit(x_train,y_train, epochs=20, batch_size=16, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b7bc6-55f5-42d0-bb18-64fcb655b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_results = model_bi.evaluate(x_test, y_test)\n",
    "bi_loss = bi_results[0]  # Extract the loss from the results\n",
    "bi_accu = (bi_results[1]*100)  # Extract the accuracy from the results\n",
    "\n",
    "print(f\"Model Loss: {bi_loss}\")\n",
    "print(f\"Model Accuracy: {bi_accu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf4fcd-b7b4-4fe1-9510-bc8cc9932d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_bi = model_bi.predict(x_test)\n",
    "y_pred_bi = (y_pred_prob_bi > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41494fc-050a-45de-b636-bc6b72a33dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(historical_bi.history)\n",
    "\n",
    "pd.DataFrame(historical_bi.history)[['accuracy', 'val_accuracy']].plot()\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "pd.DataFrame(historical_bi.history)[['loss', 'val_loss']].plot()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab2c60-5199-4677-93c1-32c2586b177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_bi = confusion_matrix(y_test,y_pred_bi)\n",
    "ax_bi = ConfusionMatrixDisplay(confusion_matrix=cnf_bi,display_labels=['Phishing','Normal'])\n",
    "ax_bi.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8594c-d2c1-4bc4-b253-72ee7f3183c0",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n",
    "\n",
    "\n",
    "A Gated Recurrent Unit (GRU) neural network is a type of RNN that uses gating mechanisms to control the flow of information, making it simpler and more computationally efficient than LSTM networks while still capturing dependencies in sequential data. As with the previous models we have explored, GRUs are commonly used in tasks such as time series forecasting, speech recognition, and text generation, where they help to maintain relevant information across sequences without the complexity of LSTM’s multiple gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5d542-f833-4dcc-8625-75404cfe698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential() # Sequential() API\n",
    "model_gru.add(Embedding(len(tk.word_index)+1,50))\n",
    "model_gru.add(GRU(units=100))\n",
    "model_gru.add(Dropout(0.5))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f838ca-00c3-453a-a5c0-3381ed6b1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06f468-1a48-4b26-9a12-400c9ec55aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_gru = model_gru.fit(x_train,y_train, epochs=20, batch_size=16, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f03fb-7a85-4170-be70-7e9791c8b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_results = model_gru.evaluate(x_test, y_test)\n",
    "gru_loss = gru_results[0]  # Extract the loss from the results\n",
    "gru_accu = (gru_results[1]*100)  # Extract the accuracy from the results\n",
    "\n",
    "print(f\"Model Loss: {gru_loss}\")\n",
    "print(f\"Model Accuracy: {gru_accu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f18b0-8afb-45ce-a7c7-e79a2a219613",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_gru = model_gru.predict(x_test)\n",
    "y_pred_gru = (y_pred_prob_gru > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8e8e9-2ac2-4c79-b51b-527edb3e782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(historical_gru.history)\n",
    "\n",
    "pd.DataFrame(historical_gru.history)[['accuracy', 'val_accuracy']].plot()\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "pd.DataFrame(historical_gru.history)[['loss', 'val_loss']].plot()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7eed0-d897-4742-abd0-14d14a63c7c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cnf_gru = confusion_matrix(y_test,y_pred_gru)\n",
    "ax_gru = ConfusionMatrixDisplay(confusion_matrix=cnf_gru,display_labels=['Pishing','normal'])\n",
    "ax_gru.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba33340-39f0-490b-8377-deb587641cad",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710203ee-a60d-4aa1-9eea-1e12b20f37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up table of models and comparisons\n",
    "\n",
    "accu_values = [srnn_accu,lstm_accu, bi_accu, gru_accu]\n",
    "row_labels = [\"Simple RNN\", \"LSTM Neural Network\", \"Bidirectional Neural Network\", \"Gated Recurrent Unit\"]\n",
    "comp_df = pd.DataFrame(accu_values, columns=[\"accuracy\"], index=row_labels)\n",
    "print(comp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502a959-b0af-4042-8bfb-4226b5026705",
   "metadata": {},
   "source": [
    "Further questions and CTF question ideas.\n",
    "- What is the accuracy of a model if we have the student train longer or add or subtract layers?\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
